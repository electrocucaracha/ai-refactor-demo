import os

import chromadb
import streamlit as st
from langchain.prompts import ChatPromptTemplate
from llama_index.core import (Document, Settings, SimpleDirectoryReader,
                              VectorStoreIndex)
from llama_index.core.node_parser import CodeSplitter
from llama_index.core.prompts import LangchainPromptTemplate
from llama_index.embeddings.ollama import OllamaEmbedding
from llama_index.llms.ollama import Ollama


def refactor(code, lang):
    with right.status("Refactoring code...", expanded=True) as status:
        # Convert string code in chunks and store them in a ChromaDB instance
        documents = [Document(text=code)]
        splitter = CodeSplitter(language=lang)
        st.write("Splitting source code...")
        nodes = splitter.get_nodes_from_documents(documents)
        st.write("Populating database...")
        index = VectorStoreIndex(nodes)

        query_engine = index.as_query_engine()

        # Customized QA prompt template
        template = """ 
         You are an expert programmer that writes simple, concise well-documented code using best practices.
         Consider the retrieved context generated by the question to produce a new function using latest python syntax with docstring and test units.
         Context: {context}
         Question: {question}"""
        lc_prompt_tmpl = LangchainPromptTemplate(
            template=ChatPromptTemplate.from_template(template),
            template_var_mappings={"query_str": "question", "context_str": "context"},
        )
        query_engine.update_prompts(
            {"response_synthesizer:text_qa_template": lc_prompt_tmpl}
        )

        query = "give me the function that starts with letter 'mul'"
        st.write("Getting suggestion...")
        response = query_engine.query(query)
        status.update(label="Refactored complete!", state="complete", expanded=False)

    return response.response


st.title("AI Code Refactor")

embedding = st.sidebar.selectbox("Ollama Embedding Model:", ("mxbai-embed-large",))
llm = st.sidebar.selectbox("Ollama Model:", ("llama3.2", "deepseek-coder"))

src_sample = """
def add_numbers(a, b):
    return a + b

def subtract_numbers(a, b):
    return a - b

def multiply_numbers(a, b):
    return a * b

def divide_numbers(a, b):
    if b != 0:
        return a / b
    return "Division by zero error"
"""

left, right = st.columns(2)
src = left.text_area("Source code:", src_sample, height=512, disabled=True)
lang = left.selectbox("Language:", ("python",), disabled=True)
right.text("Suggested code:")
if left.button("Refactor", icon="ðŸ˜ƒ", use_container_width=True):
    base_url = os.getenv("OLLAMA_BASE_URL") or "http://127.0.0.1:11434"
    Settings.embed_model = OllamaEmbedding(base_url=base_url, model_name=embedding)
    Settings.llm = Ollama(base_url=base_url, model=llm, request_timeout=360.0)
    right.markdown(refactor(src, lang))
