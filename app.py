# Copyright (c) 2025
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
# implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Provides an application for testing refactoring AI functionality
"""

import os

import streamlit as st
from langchain.prompts import ChatPromptTemplate
from llama_index.core import Document, Settings, VectorStoreIndex
from llama_index.core.node_parser import CodeSplitter
from llama_index.core.prompts import LangchainPromptTemplate
from llama_index.embeddings.ollama import OllamaEmbedding
from llama_index.llms.ollama import Ollama

import ollama


def ensure_model(model: str, host: str) -> None:
    """
    Ensure that the OLlama model is downloaded

    Args:
        model (str): Llama model name
        host (str): Ollama host name.
    """
    client = ollama.Client(host=host)
    if f"{model}:latest" not in [m.model for m in client.list()["models"]]:
        st.info(f"Downloading {model} model...")
        client.pull(model)


def refactor(code: str, language: str) -> str:
    """
    Refactor the source code provide based on the latest language updates including unit tests.

    Args:
        code (str): Source code.
        language (str): Programming Language.

    Returns:
        str: Markdown explanation of the refactored code.
    """

    with right.status("Refactoring code...", expanded=True) as status:
        # Convert string code in chunks and store them in a ChromaDB instance
        documents = [Document(text=code)]
        splitter = CodeSplitter(language=language)
        st.write("Splitting source code...")
        nodes = splitter.get_nodes_from_documents(documents)
        st.write("Populating database...")
        index = VectorStoreIndex(nodes)

        query_engine = index.as_query_engine()

        # Customized QA prompt template
        template = """
         You are an expert programmer that writes simple, concise well-documented code using best practices.
         Consider the retrieved context generated by the question to produce a new function using latest
         python syntax with docstring and test units.
         Context: {context}
         Question: {question}"""
        lc_prompt_tmpl = LangchainPromptTemplate(
            template=ChatPromptTemplate.from_template(template),
            template_var_mappings={"query_str": "question", "context_str": "context"},
        )
        query_engine.update_prompts(
            {"response_synthesizer:text_qa_template": lc_prompt_tmpl}
        )

        query = "give me the function that starts with letter 'mul'"
        st.write("Getting suggestion...")
        response = query_engine.query(query)
        status.update(label="Refactored complete!", state="complete", expanded=False)

    return response.response


st.title("AI Code Refactor")

embedding = st.sidebar.selectbox(
    "Ollama Embedding Model:", ("mxbai-embed-large", "nomic-embed-text", "all-minilm")
)
llm = st.sidebar.selectbox(
    "Ollama Model:", ("llama3.2", "qwen2.5-coder", "deepseek-coder")
)

SRC_SAMPLE = """
def add_numbers(a, b):
    return a + b

def subtract_numbers(a, b):
    return a - b

def multiply_numbers(a, b):
    return a * b

def divide_numbers(a, b):
    if b != 0:
        return a / b
    return "Division by zero error"
"""

left, right = st.columns(2)
src = left.text_area("Source code:", SRC_SAMPLE, height=512, disabled=True)
lang = left.selectbox("Language:", ("python",), disabled=True)
if left.button("Refactor", icon="ðŸ˜ƒ", use_container_width=True):
    base_url = os.getenv("OLLAMA_BASE_URL") or "http://127.0.0.1:11434"
    ensure_model(embedding, base_url)
    ensure_model(llm, base_url)
    Settings.embed_model = OllamaEmbedding(base_url=base_url, model_name=embedding)
    Settings.llm = Ollama(base_url=base_url, model=llm, request_timeout=360.0)
    right.markdown(refactor(src, lang))
    st.success("Done!!!")
